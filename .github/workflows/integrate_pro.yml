name: Integrate Pro (build training tables)

on:
  workflow_dispatch:
    inputs:
      start_date:
        description: "開始日 (YYYYMMDD)"
        required: true
        default: "20240225"
      end_date:
        description: "終了日 (YYYYMMDD)"
        required: true
        default: "20240229"
      out_format:
        description: "出力形式 (csv / parquet / both)"
        required: true
        default: "csv"
      csv_compress:
        description: "CSVをgzip圧縮（.csv.gz）する"
        required: true
        default: "true"

permissions:
  contents: write

concurrency:
  group: integrate-pro-${{ github.repository }}-${{ github.ref_name }}
  cancel-in-progress: false

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install polars pyarrow

      # 範囲内で実在するプログラム日を検出
      - name: Discover available dates within range
        id: discover
        env:
          START: ${{ github.event.inputs.start_date }}
          END: ${{ github.event.inputs.end_date }}
        run: |
          set -euo pipefail
          START="${START}"
          END="${END}"

          # 期待構造: public/programs/v1/YYYY/MMDD/
          mapfile -t ALL_DIRS < <(find public/programs/v1 -mindepth 2 -maxdepth 2 -type d 2>/dev/null | sort || true)
          DATES=()
          for d in "${ALL_DIRS[@]}"; do
            y=$(basename "$(dirname "$d")")
            md=$(basename "$d")
            if [[ "$y" =~ ^[0-9]{4}$ && "$md" =~ ^[0-9]{4}$ ]]; then
              hd="${y}${md}"
              if [[ "$hd" -ge "$START" && "$hd" -le "$END" ]]; then
                DATES+=("$hd")
              fi
            fi
          done

          if [ "${#DATES[@]}" -eq 0 ]; then
            echo "No program dirs found within range ${START}-${END}."
            echo "found_dates=" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          uniq_sorted=$(printf "%s\n" "${DATES[@]}" | sort -u | tr '\n' ' ')
          echo "Found dates: ${uniq_sorted}"
          echo "found_dates=${uniq_sorted}" >> "$GITHUB_OUTPUT"

      # 日別シャードで出力（上書き防止）
      - name: Integrate (daily shards; no overwrite)
        if: steps.discover.outputs.found_dates != ''
        env:
          DATES: ${{ steps.discover.outputs.found_dates }}
          OUT_FMT: ${{ github.event.inputs.out_format }}
          CSV_COMPRESS: ${{ github.event.inputs.csv_compress }}
        run: |
          set -euo pipefail
          CSV_COMP_FLAG=""
          if [ "${CSV_COMPRESS,,}" = "true" ]; then
            CSV_COMP_FLAG="--csv_compress"
          fi

          for hd in ${DATES}; do
            YEAR="${hd:0:4}"
            MD="${hd:4:4}"
            PDIR="public/programs/v1/${YEAR}/${MD}"
            OUTDIR="data/shards/${YEAR}/${MD}"

            echo "===> Processing ${YEAR}/${MD}"
            python scripts/integrate_pro.py \
              --program_dir "${PDIR}" \
              --exhibition_dir public/exhibition/v1 \
              --results_dir public/results \
              --racer_dir public/racers-annual \
              --out_dir "${OUTDIR}" \
              --out_format "${OUT_FMT}" \
              ${CSV_COMP_FLAG}
          done

      # シャードを結合して単一テーブルを生成（.csv.gz対応）
      - name: Merge shards -> single tables
        if: steps.discover.outputs.found_dates != ''
        env:
          OUT_FMT: ${{ github.event.inputs.out_format }}
          CSV_COMPRESS: ${{ github.event.inputs.csv_compress }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import polars as pl
          from pathlib import Path

          def read_csv_auto(p: Path) -> pl.DataFrame:
            sp = str(p)
            if sp.endswith(".gz"):
              return pl.read_csv(p, compression="gzip", infer_schema_length=10000)
            return pl.read_csv(p, infer_schema_length=10000)

          def write_csv(df: pl.DataFrame, path: Path, compress: bool):
            path.parent.mkdir(parents=True, exist_ok=True)
            if compress and not str(path).endswith(".gz"):
              path = Path(str(path) + ".gz")
            df.write_csv(path)
            print(f"[WRITE] {path} (rows={df.height})")

          shards_root = Path("data/shards")
          out_root = Path("data")
          out_root.mkdir(parents=True, exist_ok=True)

          out_fmt = "${{ github.event.inputs.out_format }}".lower()
          csv_compress = "${{ github.event.inputs.csv_compress }}".lower() == "true"

          # integrated_pro
          if out_fmt in ("csv","both"):
            parts = sorted(shards_root.glob("*/" + "*" + "/integrated_pro.csv*"))
            if parts:
              dfs = []
              for p in parts:
                try:
                  dfs.append(read_csv_auto(p))
                except Exception as e:
                  print(f"[WARN] read_csv failed: {p} ({e})")
              if dfs:
                df = pl.concat(dfs, how="vertical", rechunk=True)
                df = df.unique(subset=["hd","jcd","rno","lane"], keep="last")
                write_csv(df, out_root/"integrated_pro.csv", csv_compress)
              else:
                print("[MERGE] integrated_pro: no readable CSV shards")
            else:
              print("[MERGE] integrated_pro: no CSV shards found")

          if out_fmt in ("parquet","both"):
            parts = sorted(shards_root.glob("*/" + "*" + "/integrated_pro.parquet"))
            if parts:
              dfs = []
              for p in parts:
                try:
                  dfs.append(pl.read_parquet(p))
                except Exception as e:
                  print(f"[WARN] read_parquet failed: {p} ({e})")
              if dfs:
                df = pl.concat(dfs, how="vertical", rechunk=True)
                df = df.unique(subset=["hd","jcd","rno","lane"], keep="last")
                (out_root/"integrated_pro.parquet").parent.mkdir(parents=True, exist_ok=True)
                df.write_parquet(out_root/"integrated_pro.parquet")
                print(f"[WRITE] {out_root/'integrated_pro.parquet'} (rows={df.height})")
              else:
                print("[MERGE] integrated_pro: no readable Parquet shards")
            else:
              print("[MERGE] integrated_pro: no Parquet shards found")

          # train_120_pro
          if out_fmt in ("csv","both"):
            parts = sorted(shards_root.glob("*/" + "*" + "/train_120_pro.csv*"))
            if parts:
              dfs = []
              for p in parts:
                try:
                  dfs.append(read_csv_auto(p))
                except Exception as e:
                  print(f("[WARN] read_csv failed: {p} ({e})"))
              if dfs:
                df = pl.concat(dfs, how="vertical", rechunk=True)
                df = df.unique(subset=["hd","jcd","rno","combo"], keep="last")
                write_csv(df, out_root/"train_120_pro.csv", csv_compress)
              else:
                print("[MERGE] train_120_pro: no readable CSV shards")
            else:
              print("[MERGE] train_120_pro: no CSV shards found")

          if out_fmt in ("parquet","both"):
            parts = sorted(shards_root.glob("*/" + "*" + "/train_120_pro.parquet"))
            if parts:
              dfs = []
              for p in parts:
                try:
                  dfs.append(pl.read_parquet(p))
                except Exception as e:
                  print(f"[WARN] read_parquet failed: {p} ({e})")
              if dfs:
                df = pl.concat(dfs, how="vertical", rechunk=True)
                df = df.unique(subset=["hd","jcd","rno","combo"], keep="last")
                (out_root/"train_120_pro.parquet").parent.mkdir(parents=True, exist_ok=True)
                df.write_parquet(out_root/"train_120_pro.parquet")
                print(f"[WRITE] {out_root/'train_120_pro.parquet'} (rows={df.height})")
              else:
                print("[MERGE] train_120_pro: no readable Parquet shards")
            else:
              print("[MERGE] train_120_pro: no Parquet shards found")
          PY

      - name: Commit & push results (if changed)
        if: steps.discover.outputs.found_dates != ''
        run: |
          set -euo pipefail
          git config user.name  "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add -A data || true
          if ! git diff --cached --quiet; then
            git commit -m "integrate_pro: merged shards for ${{ github.event.inputs.start_date }}-${{ github.event.inputs.end_date }} (${{ steps.discover.outputs.found_dates }})"
            git push
          else
            echo "No changes to commit."
          fi

      - name: Nothing to do (no dates)
        if: steps.discover.outputs.found_dates == ''
        run: |
          echo "No program dates found in the specified range. Adjust start/end or fetch programs first."
